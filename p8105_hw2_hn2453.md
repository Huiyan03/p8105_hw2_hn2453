p8105_hw2_hn2453
================
Huiyan Ni
2024-10-02

## problem 1

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
library(readr)
```

Import the data related to each entrance and exit for each subway
station in NYC. Read and clean the data; retain line, station, name,
station latitude / longitude, routes served, entry, vending, entrance
type, and ADA compliance.

``` r
nyc_sub = read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
                   col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c"))

nyc_sub = nyc_sub |> janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

After importing original dataset, making route variables to character
variable, selecting line, station_name, station_latitude,
station_longitude, all route variables, entry, exit_only, vending,
entrance_type, and ada columns, and convert the entry variable from
character to a logical variable, now the dataset is 1868 \* 20.

How many distinct stations are there?

``` r
nyc_sub |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

There are totally 465 rows, thus, there are 465 distinct stations.

How many stations are ADA compliant?

``` r
nyc_sub |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

There are totally 84 rows, thus, 84 distinct stations are ADA compliant.

What proportion of station entrances / exits without vending allow
entrance?

``` r
nyc_sub |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```

    ## [1] 0.3770492

The proportion of station entrances without vending allow entrance is
about 0.377.

Reformat data so that route number and route name are distinct
variables.How many distinct stations serve the A train? Of the stations
that serve the A train, how many are ADA compliant?

``` r
nyc_sub |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # ℹ 50 more rows

``` r
nyc_sub |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

60 distinct stations serve the A train, and 17 stations that serve the A
train are ADA compliant.

## problem 2

Read and clean the Mr. Trash Wheel sheet. Specify the import sheet is
“Mr. Trash Wheel”, skip the first row due to the rows is non-data,
define blank as na, convert variable names to reasonable variable names,
omit rows that do not include dumpster-specific data(value is na), round
the number of sports balls to the nearest integer and converts the
result to an integer variable, and remove 2 useless columns producing
after import data.

``` r
mr_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      sheet = "Mr. Trash Wheel",
                      col_names = TRUE,
                      skip = 1,
                      na = c("","NA")) |> 
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(sports_balls = as.integer(round(sports_balls)))
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
 mr_trash = mr_trash |> select(-x15,-x16)
 head(mr_trash)
```

    ## # A tibble: 6 × 14
    ##   dumpster month year  date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <chr> <dttm>                    <dbl>              <dbl>
    ## 1        1 May   2014  2014-05-16 00:00:00        4.31                 18
    ## 2        2 May   2014  2014-05-16 00:00:00        2.74                 13
    ## 3        3 May   2014  2014-05-16 00:00:00        3.45                 15
    ## 4        4 May   2014  2014-05-17 00:00:00        3.1                  15
    ## 5        5 May   2014  2014-05-17 00:00:00        4.06                 18
    ## 6        6 May   2014  2014-05-20 00:00:00        2.71                 13
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>

Use a similar process to import, clean, and organize the data for
Professor Trash Wheel and Gwynnda.

``` r
pro_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      sheet = "Professor Trash Wheel",
                      col_names = TRUE,
                      skip = 1,
                      na = c("","NA")) |> 
  janitor::clean_names() |>
  filter(!is.na(dumpster))
  
 head(pro_trash)
```

    ## # A tibble: 6 × 13
    ##   dumpster month     year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 January   2017 2017-01-02 00:00:00        1.79                 15
    ## 2        2 January   2017 2017-01-30 00:00:00        1.58                 15
    ## 3        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ## 4        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ## 5        5 February  2017 2017-02-28 00:00:00        1.45                 15
    ## 6        6 March     2017 2017-03-30 00:00:00        1.71                 15
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>

``` r
gwy_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      sheet = "Gwynnda Trash Wheel",
                      col_names = TRUE,
                      skip = 1,
                      na = c("","NA")) |> 
  janitor::clean_names() |>
  filter(!is.na(dumpster))

head(gwy_trash)
```

    ## # A tibble: 6 × 12
    ##   dumpster month   year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        1 July    2021 2021-07-03 00:00:00        0.93                 15
    ## 2        2 July    2021 2021-07-07 00:00:00        2.26                 15
    ## 3        3 July    2021 2021-07-07 00:00:00        1.62                 15
    ## 4        4 July    2021 2021-07-16 00:00:00        1.76                 15
    ## 5        5 July    2021 2021-07-30 00:00:00        1.53                 15
    ## 6        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ## # ℹ 6 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>

Add an additional variable to note different trash wheel before
combining these data sets.

``` r
mr_trash = mr_trash |> mutate(wheel_name = "Mr. Trash Wheel")
pro_trash = pro_trash |> mutate(wheel_name = "Professor Trash Wheel")
gwy_trash = gwy_trash |> mutate(wheel_name = "Gwynnda Trash Wheel")
```

Combining these data sets.

``` r
mr_trash$year = as.numeric(mr_trash$year)
trash_wheel = bind_rows(mr_trash,pro_trash,gwy_trash)
trash_wheel
```

    ## # A tibble: 1,033 × 15
    ##    dumpster month  year date                weight_tons volume_cubic_yards
    ##       <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ##  1        1 May    2014 2014-05-16 00:00:00        4.31                 18
    ##  2        2 May    2014 2014-05-16 00:00:00        2.74                 13
    ##  3        3 May    2014 2014-05-16 00:00:00        3.45                 15
    ##  4        4 May    2014 2014-05-17 00:00:00        3.1                  15
    ##  5        5 May    2014 2014-05-17 00:00:00        4.06                 18
    ##  6        6 May    2014 2014-05-20 00:00:00        2.71                 13
    ##  7        7 May    2014 2014-05-21 00:00:00        1.91                  8
    ##  8        8 May    2014 2014-05-28 00:00:00        3.7                  16
    ##  9        9 June   2014 2014-06-05 00:00:00        2.52                 14
    ## 10       10 June   2014 2014-06-11 00:00:00        3.76                 18
    ## # ℹ 1,023 more rows
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>, wheel_name <chr>

From the tibble, we have 1033 rows, and 15 variables: dumpster, month,
year, date, weight_tons, volume_cubic_yard, plastic_bottles,
polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers,
sports_balls, homes_powered, and wheel_name.

What was the total weight of trash collected by Professor Trash Wheel?

``` r
weight_pro = trash_wheel |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  pull(weight_tons) |> 
  sum(na.rm=TRUE)
```

The total weight of trash collected by Professor Trash Wheel is 246.74
tons.

What was the total number of cigarette butts collected by Gwynnda in
June of 2022?

``` r
cigarette_gwy = trash_wheel |>
  filter(wheel_name == "Gwynnda Trash Wheel") |>
  filter(month == "June") |>
  filter(year == "2022") |>
  pull(cigarette_butts) |>
  sum(na.rm=TRUE)

cigarette_gwy
```

    ## [1] 18120

The total number of cigarette butts collected by Gwynnda in June of 2022
is 1.812^{4}.

## problem 3

Import and create three data sets from three different excels.

``` r
bake = 
  read_csv("data/gbb_datasets/bakes.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

head(bake)
```

    ## # A tibble: 6 × 5
    ##   series episode baker     signature_bake                           show_stopper
    ##    <dbl>   <dbl> <chr>     <chr>                                    <chr>       
    ## 1      1       1 Annetha   Light Jamaican Black Cakewith Strawberr… Red, White …
    ## 2      1       1 David     Chocolate Orange Cake                    Black Fores…
    ## 3      1       1 Edd       Caramel Cinnamon and Banana Cake         <NA>        
    ## 4      1       1 Jasminder Fresh Mango and Passion Fruit Hummingbi… <NA>        
    ## 5      1       1 Jonathan  Carrot Cake with Lime and Cream Cheese … Three Tiere…
    ## 6      1       1 Lea       Cranberry and Pistachio Cakewith Orange… Raspberries…

``` r
baker = 
  read_csv("data/gbb_datasets/bakers.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

head(baker)
```

    ## # A tibble: 6 × 5
    ##   baker_name       series baker_age baker_occupation   hometown                 
    ##   <chr>             <dbl>     <dbl> <chr>              <chr>                    
    ## 1 Ali Imdad             4        25 Charity worker     Saltley, Birmingham      
    ## 2 Alice Fevronia       10        28 Geography teacher  Essex                    
    ## 3 Alvin Magallanes      6        37 Nurse              Bracknell, Berkshire     
    ## 4 Amelia LeBruin       10        24 Fashion designer   Halifax                  
    ## 5 Andrew Smyth          7        25 Aerospace engineer Derby / Holywood, County…
    ## 6 Annetha Mills         1        30 Midwife            Essex

``` r
result = 
  read_csv("data/gbb_datasets/results.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      skip = 2,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

head(result)
```

    ## # A tibble: 6 × 5
    ##   series episode baker     technical result
    ##    <dbl>   <dbl> <chr>         <dbl> <chr> 
    ## 1      1       1 Annetha           2 IN    
    ## 2      1       1 David             3 IN    
    ## 3      1       1 Edd               1 IN    
    ## 4      1       1 Jasminder        NA IN    
    ## 5      1       1 Jonathan          9 IN    
    ## 6      1       1 Louise           NA IN

Finding the baker name in bakes.csv and results.csv is just first name,
but the baker name in bakers.csv is full name. We need create a new
variable for baker dataset in order to combine three data sets together.

``` r
baker$baker <- sapply(strsplit(baker$baker_name, " "), `[`, 1)
head(baker)
```

    ## # A tibble: 6 × 6
    ##   baker_name       series baker_age baker_occupation   hometown            baker
    ##   <chr>             <dbl>     <dbl> <chr>              <chr>               <chr>
    ## 1 Ali Imdad             4        25 Charity worker     Saltley, Birmingham Ali  
    ## 2 Alice Fevronia       10        28 Geography teacher  Essex               Alice
    ## 3 Alvin Magallanes      6        37 Nurse              Bracknell, Berkshi… Alvin
    ## 4 Amelia LeBruin       10        24 Fashion designer   Halifax             Amel…
    ## 5 Andrew Smyth          7        25 Aerospace engineer Derby / Holywood, … Andr…
    ## 6 Annetha Mills         1        30 Midwife            Essex               Anne…

Create a single, well-organized dataset with all the information
contained in these data sets.

``` r
bake_result = full_join(bake, result, by = c("series","episode","baker"))

bake_total = full_join(bake_result, baker, by = c("baker","series"))
bake_total
```

    ## # A tibble: 1,145 × 11
    ##    series episode baker  signature_bake show_stopper technical result baker_name
    ##     <dbl>   <dbl> <chr>  <chr>          <chr>            <dbl> <chr>  <chr>     
    ##  1      1       1 Annet… "Light Jamaic… Red, White …         2 IN     Annetha M…
    ##  2      1       1 David  "Chocolate Or… Black Fores…         3 IN     David Cha…
    ##  3      1       1 Edd    "Caramel Cinn… <NA>                 1 IN     Edd Kimber
    ##  4      1       1 Jasmi… "Fresh Mango … <NA>                NA IN     Jasminder…
    ##  5      1       1 Jonat… "Carrot Cake … Three Tiere…         9 IN     Jonathan …
    ##  6      1       1 Lea    "Cranberry an… Raspberries…        10 OUT    Lea Harris
    ##  7      1       1 Louise "Carrot and O… Never Fail …        NA IN     Louise Br…
    ##  8      1       1 Mark   "Sticky Marma… Heart-shape…        NA OUT    Mark Whit…
    ##  9      1       1 Miran… "Triple Layer… Three Tiere…         8 IN     Miranda B…
    ## 10      1       1 Ruth   "Three Tiered… Classic Cho…        NA IN     Ruth Clem…
    ## # ℹ 1,135 more rows
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

Check the completeness of the final dataset.

``` r
bake_na = anti_join(bake, bake_total, by = c("series","episode","baker"))
bake_na
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
baker_na = anti_join(baker, bake_total, by = c("series","baker"))
baker_na
```

    ## # A tibble: 0 × 6
    ## # ℹ 6 variables: baker_name <chr>, series <dbl>, baker_age <dbl>,
    ## #   baker_occupation <chr>, hometown <chr>, baker <chr>

``` r
result_na = anti_join(result, bake_total, by = c("series","episode","baker"))
result_na
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, baker <chr>, technical <dbl>,
    ## #   result <chr>

All three datasets are null, thus, the completeness no problem.

Check the correctness of the final dataset.

``` r
bake_total |> filter(series =="1",episode == "2",baker=="Edd")
```

    ## # A tibble: 1 × 11
    ##   series episode baker signature_bake   show_stopper technical result baker_name
    ##    <dbl>   <dbl> <chr> <chr>            <chr>            <dbl> <chr>  <chr>     
    ## 1      1       2 Edd   Oatmeal Raisin … Pink Macaro…         6 IN     Edd Kimber
    ## # ℹ 3 more variables: baker_age <dbl>, baker_occupation <chr>, hometown <chr>

By checking from three original excel, the information is correct, the
correctness no problem.

The series and episode in final dataset are increasing order. This is
easy for read and find.

Export the result as a CSV in the directory containing the original
datasets.

``` r
write.csv(bake_total, file = "data/gbb_datasets/bake_total.csv", row.names = FALSE)
```

Create a reader-friendly table showing the star baker or winner of each
episode in Seasons 5 through 10.

``` r
result_5_to_10 = result |> 
  filter(4 < series & series < 11) |>
  filter(result %in% c("WINNER","STAR BAKER"))

result_5_to_10
```

    ## # A tibble: 60 × 5
    ##    series episode baker   technical result    
    ##     <dbl>   <dbl> <chr>       <dbl> <chr>     
    ##  1      5       1 Nancy           1 STAR BAKER
    ##  2      5       2 Richard         1 STAR BAKER
    ##  3      5       3 Luis            2 STAR BAKER
    ##  4      5       4 Richard         5 STAR BAKER
    ##  5      5       5 Kate            3 STAR BAKER
    ##  6      5       6 Chetna          2 STAR BAKER
    ##  7      5       7 Richard         1 STAR BAKER
    ##  8      5       8 Richard         4 STAR BAKER
    ##  9      5       9 Richard         2 STAR BAKER
    ## 10      5      10 Nancy           1 WINNER    
    ## # ℹ 50 more rows

From the table, we can get every episodes from 1 to 9 have one star
baker, and the final winner for each series will uncover in episode 10.
I think the final winner is unpredictable according to this table. For
example, in series 10, Steph got 4 times star baker, but the final
winner is David who haven’t get any star baker before episode 10 in this
series.

Import, clean, tidy, and organize the viewership data in viewers.csv

``` r
viewer = 
  read_csv("data/gbb_datasets/viewers.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 
```

Show the first 10 rows of this dataset.

``` r
head(viewer,10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

What was the average viewership in Season 1?

``` r
season_1 = viewer |> 
  filter(episode == "1")|>
  select(starts_with("series"))

season_1
```

    ## # A tibble: 1 × 10
    ##   series_1 series_2 series_3 series_4 series_5 series_6 series_7 series_8
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ## 1     2.24      3.1     3.85      6.6     8.51     11.6     13.6     9.46
    ## # ℹ 2 more variables: series_9 <dbl>, series_10 <dbl>

``` r
season_1_mean = mean(as.numeric(unlist(season_1)))
```

The average viewership in Season 1 is 7.813

In Season 5?

``` r
season_5 = viewer |> 
  filter(episode == "5")|>
  select(starts_with("series"))

season_5
```

    ## # A tibble: 1 × 10
    ##   series_1 series_2 series_3 series_4 series_5 series_6 series_7 series_8
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ## 1     3.03     3.83     4.61     6.95     9.95     12.4     13.1     8.61
    ## # ℹ 2 more variables: series_9 <dbl>, series_10 <dbl>

``` r
season_5_mean = mean(as.numeric(unlist(season_5)))
```

The average viewership in Season 5 is 8.042
