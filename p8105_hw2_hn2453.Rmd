---
title: "p8105_hw2_hn2453"
author: "Huiyan Ni"
date: "2024-10-02"
output: github_document
---

## problem 1

```{r}
library(tidyverse)
library(readxl)
library(readr)
```

Import the data related to each entrance and exit for each subway station in NYC.
Read and clean the data; retain line, station, name, station latitude / longitude, routes served, entry, vending, entrance type, and ADA compliance.
```{r}
nyc_sub = read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
                   col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c"))

nyc_sub = nyc_sub |> janitor::clean_names() |> 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) |> 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))

```
After importing original dataset, making route variables to character variable, selecting line, station_name, station_latitude, station_longitude, all route variables, entry, exit_only, vending, entrance_type, and ada columns, and convert the entry variable from character to a logical variable, now the dataset is 1868 * 20.   

How many distinct stations are there?
```{r}
nyc_sub |> 
  select(station_name, line) |> 
  distinct()
```
There are totally 465 rows, thus, there are 465 distinct stations.

How many stations are ADA compliant?
```{r}
nyc_sub |> 
  filter(ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()
```
There are totally 84 rows, thus, 84 distinct stations are ADA compliant.

What proportion of station entrances / exits without vending allow entrance?
```{r}
nyc_sub |> 
  filter(vending == "NO") |> 
  pull(entry) |> 
  mean()
```
The proportion of station entrances without vending allow entrance is about 0.377.

Reformat data so that route number and route name are distinct variables.How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?
```{r}
nyc_sub |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A") |> 
  select(station_name, line) |> 
  distinct()

nyc_sub |> 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") |> 
  filter(route == "A", ada == TRUE) |> 
  select(station_name, line) |> 
  distinct()

```
60 distinct stations serve the A train, and 17 stations that serve the A train are ADA compliant.

## problem 2

Read and clean the Mr. Trash Wheel sheet. Specify the import sheet is "Mr. Trash Wheel", skip the first row due to the rows is non-data, define blank as na, convert variable names to reasonable variable names, omit rows that do not include dumpster-specific data(value is na), round the number of sports balls to the nearest integer and converts the result to an integer variable, and remove 2 useless columns producing after import data.
```{r}
mr_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      sheet = "Mr. Trash Wheel",
                      col_names = TRUE,
                      skip = 1,
                      na = c("","NA")) |> 
  janitor::clean_names() |>
  filter(!is.na(dumpster)) |>
  mutate(sports_balls = as.integer(round(sports_balls)))
  
 mr_trash = mr_trash |> select(-x15,-x16)
 head(mr_trash)
```
Use a similar process to import, clean, and organize the data for Professor Trash Wheel and Gwynnda.
```{r}
pro_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      sheet = "Professor Trash Wheel",
                      col_names = TRUE,
                      skip = 1,
                      na = c("","NA")) |> 
  janitor::clean_names() |>
  filter(!is.na(dumpster))
  
 head(pro_trash)
```
```{r}
gwy_trash = 
  read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      sheet = "Gwynnda Trash Wheel",
                      col_names = TRUE,
                      skip = 1,
                      na = c("","NA")) |> 
  janitor::clean_names() |>
  filter(!is.na(dumpster))

head(gwy_trash)
```

Add an additional variable to note different trash wheel before combining these data sets.
```{r}
mr_trash = mr_trash |> mutate(wheel_name = "Mr. Trash Wheel")
pro_trash = pro_trash |> mutate(wheel_name = "Professor Trash Wheel")
gwy_trash = gwy_trash |> mutate(wheel_name = "Gwynnda Trash Wheel")

```

Combining these data sets.
```{r}
mr_trash$year = as.numeric(mr_trash$year)
trash_wheel = bind_rows(mr_trash,pro_trash,gwy_trash)
trash_wheel
```
From the tibble, we have 1033 rows, and 15 variables: dumpster, month, year, date, weight_tons, volume_cubic_yard, plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags, wrappers, sports_balls, homes_powered, and wheel_name. 

What was the total weight of trash collected by Professor Trash Wheel?
```{r}
weight_pro = trash_wheel |> 
  filter(wheel_name == "Professor Trash Wheel") |> 
  pull(weight_tons) |> 
  sum(na.rm=TRUE)
```

The total weight of trash collected by Professor Trash Wheel is `r weight_pro` tons. 

What was the total number of cigarette butts collected by Gwynnda in June of 2022?
```{r}
cigarette_gwy = trash_wheel |>
  filter(wheel_name == "Gwynnda Trash Wheel") |>
  filter(month == "June") |>
  filter(year == "2022") |>
  pull(cigarette_butts) |>
  sum(na.rm=TRUE)

cigarette_gwy
```
The total number of cigarette butts collected by Gwynnda in June of 2022 is `r cigarette_gwy`.

## problem 3

Import and create three data sets from three different excels. 
```{r}
bake = 
  read_csv("data/gbb_datasets/bakes.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

head(bake)

baker = 
  read_csv("data/gbb_datasets/bakers.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

head(baker)

result = 
  read_csv("data/gbb_datasets/results.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      skip = 2,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

head(result)

```
Finding the baker name in bakes.csv and results.csv is just first name, but the baker name in bakers.csv is full name. We need create a new variable for baker dataset in order to combine three data sets together. 
```{r}
baker$baker <- sapply(strsplit(baker$baker_name, " "), `[`, 1)
head(baker)
```

Create a single, well-organized dataset with all the information contained in these data sets.
```{r}
bake_result = full_join(bake, result, by = c("series","episode","baker"))

bake_total = full_join(bake_result, baker, by = c("baker","series"))
bake_total

```
Check the completeness of the final dataset.
```{r}
bake_na = anti_join(bake, bake_total, by = c("series","episode","baker"))
bake_na

baker_na = anti_join(baker, bake_total, by = c("series","baker"))
baker_na

result_na = anti_join(result, bake_total, by = c("series","episode","baker"))
result_na
```
All three datasets are null, thus, the completeness no problem. 

Check the correctness of the final dataset.
```{r}
bake_total |> filter(series =="1",episode == "2",baker=="Edd")
```
By checking from three original excel, the information is correct, the correctness no problem.

The series and episode in final dataset are increasing order. This is easy for read and find.   

Export the result as a CSV in the directory containing the original datasets.
```{r}
write.csv(bake_total, file = "data/gbb_datasets/bake_total.csv", row.names = FALSE)
```

Create a reader-friendly table showing the star baker or winner of each episode in Seasons 5 through 10.
```{r}
result_5_to_10 = result |> 
  filter(4 < series & series < 11) |>
  filter(result %in% c("WINNER","STAR BAKER"))

result_5_to_10
```
From the table, we can get every episodes from 1 to 9 have one star baker, and the final winner for each series will uncover in episode 10. I think the final winner is unpredictable according to this table. For example, in series 10, Steph got 4 times star baker, but the final winner is David who haven't get any star baker before episode 10 in this series. 

Import, clean, tidy, and organize the viewership data in viewers.csv
```{r}
viewer = 
  read_csv("data/gbb_datasets/viewers.csv", 
                      col_names = TRUE,
                      show_col_types = FALSE,
                      na = c("","NA","N/A")) |> 
  janitor::clean_names() 

```

Show the first 10 rows of this dataset.
```{r}
head(viewer,10)
```

What was the average viewership in Season 1? 
```{r}
season_1 = viewer |> 
  filter(episode == "1")|>
  select(starts_with("series"))

season_1

season_1_mean = mean(as.numeric(unlist(season_1)))
```
The average viewership in Season 1 is `r season_1_mean `

In Season 5?
```{r}
season_5 = viewer |> 
  filter(episode == "5")|>
  select(starts_with("series"))

season_5

season_5_mean = mean(as.numeric(unlist(season_5)))
```
The average viewership in Season 5 is `r season_5_mean `